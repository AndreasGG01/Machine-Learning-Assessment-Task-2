{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Perceptron - Pima Indians Diabetes Classification"
      ],
      "metadata": {
        "id": "gBhavBZ-zALa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare Packages and Google Collab"
      ],
      "metadata": {
        "id": "Y6Z86nhs0Npu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wL8d2Z9Oyj6K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mount the google drive to access the data \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc0OsATM1Lun",
        "outputId": "58f76d83-ea55-45a4-b268-23567fe4d1b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialise Dataset"
      ],
      "metadata": {
        "id": "KSqnkrUQ0ViI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Real World Dataset"
      ],
      "metadata": {
        "id": "p1U6ZwQuIK32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Dataset\n",
        "data = pd.read_csv('diabetes.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "hog5j8E80Y3Y",
        "outputId": "6fa66d33-ed64-4ef1-f73f-904c530bf9e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3324604878a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'diabetes.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'diabetes.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove classification column\n",
        "variables = data.drop(\"Outcome\", axis = 1)\n",
        "#Create new data frame with classification column\n",
        "classification = data[\"Outcome\"]\n",
        "#Normalize data\n",
        "variables = MinMaxScaler().fit_transform(variables)\n",
        "#Turn Data back into a dataframe\n",
        "variables = pd.DataFrame(variables)\n",
        "variables.head()"
      ],
      "metadata": {
        "id": "6HIj33v_7VUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification.head()"
      ],
      "metadata": {
        "id": "z6O3nVtV9H9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split dataset into test and training sets\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(variables, classification, test_size=0.2, random_state=0)\n",
        "#Check the size of the training and testing sets\n",
        "print(train_X.shape)\n",
        "print(test_X.shape)\n",
        "print(train_Y.shape)\n",
        "print(test_Y.shape)"
      ],
      "metadata": {
        "id": "ZAXpxMbf4s9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert dataset into numpy arrays\n",
        "train_X = np.array(train_X)\n",
        "test_X = np.array(test_X)\n",
        "train_Y = np.array(train_Y)\n",
        "test_Y = np.array(test_Y)"
      ],
      "metadata": {
        "id": "4A2t-daHrjhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Toy Dataset"
      ],
      "metadata": {
        "id": "G448pFaUIRvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the toy dataset the iris dataset was used with data some data preparation demonstrated by Jun Li in his week 10 demonstration (https://colab.research.google.com/drive/1sdyzxla7RjCCrRWmlHVUXnIRtfyK4gCs?usp=sharing). The goal of the dataset is to simplify the iris dataset into a binary dataset with only 100 data points in order for the Perceptron to be tested on an easy data set to validate the algorithm works as expected"
      ],
      "metadata": {
        "id": "09seUU46IjZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "data = load_iris()\n",
        "#Minimise the X dataset into 2 data attributes and reduce the dataset to only 100 samples\n",
        "X = data['data'][:100, :2]\n",
        "#Take the first 100 target samples to match with the X attributes\n",
        "y = data['target'][:100]\n",
        "#Split the data set into training and testing set with 33% going to the test side\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "dvK0xao-IfP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Design the Perceptron"
      ],
      "metadata": {
        "id": "js52_nrj1PqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "  #Create the constructor for the Perceptron and initialise parameters\n",
        "  def __init__(self, learning_rate = 0.1, epochs = 10):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epochs = epochs\n",
        "    self.weights = None\n",
        "    self.bias = None\n",
        "  \n",
        "  #create an activation function\n",
        "  def activation_func(self, x):\n",
        "    #if x is greater or equal to 0 return 1 otherwise return 0 meaning if it is activated or not\n",
        "    return 1 if x >= self.bias else 0\n",
        "  \n",
        "  #create the weighted sum for the hypothesis space\n",
        "  def weighted_sum(self, x):\n",
        "    #transpose of weights times the data samples plus the bias\n",
        "    return np.dot(x, self.weights) + self.bias\n",
        "\n",
        "  #Fit the data to the model\n",
        "  def fit(self, X, y):\n",
        "    #Initialise features\n",
        "    #make the weights initially all zero based on he number of features in the data\n",
        "    self.weights = np.zeros(X.shape[1])\n",
        "    #initialise the bias to be 0\n",
        "    self.bias = 0\n",
        "    #initialise the count to be 0\n",
        "    count = 0\n",
        "    #covert labelled data into a numpy array for processing\n",
        "    y_array = np.array(y)\n",
        "\n",
        "    while(count <= self.epochs):\n",
        "      #initialise an array to append the current predicted values to\n",
        "      predict_array = []\n",
        "      for i, x in enumerate(X):\n",
        "        #calculate predicted values\n",
        "        #print(self.weights)\n",
        "        #print(x)\n",
        "        linear_calc = self.weighted_sum(x)\n",
        "        predicted_value = self.activation_func(linear_calc)\n",
        "        #append predictins for the current epoch into an array\n",
        "        predict_array.append(predicted_value)\n",
        "\n",
        "        # print(\"linear calc:\", linear_calc)\n",
        "        # print(\"real value:\", y_array[i])\n",
        "        # print(\"predicted_value:\", predicted_value)\n",
        "        \n",
        "        #Update the wegihts and biases based on an update rule\n",
        "        #Update rule stating that if the predicted value is false but the real value is true increase the weights and decrease the bias based on the learning rate\n",
        "        if y_array[i] == 1 and predicted_value == 0:\n",
        "          self.weights = self.weights + self.learning_rate * x\n",
        "          self.bias = self.bias - self.learning_rate\n",
        "        #Update rule stating that if the predicted value is true but the real value is false increase the bias and decrease the weights based on the learning rate\n",
        "        elif y_array[i] == 0 and predicted_value == 1:\n",
        "          self.weights = self.weights - self.learning_rate * x\n",
        "          self.bias = self.bias + self.learning_rate\n",
        "        \n",
        "        # print(\"weights\" , self.weights)\n",
        "        # print(\"bias\", self.bias)\n",
        "      #calculate the accuracy of the current epoch \n",
        "      curr_accuracy = accuracy_score(y_array, predict_array)\n",
        "      print(\"No. Epoch:\", count, \",\", \"Accuracy:\", curr_accuracy)\n",
        "      #increase the count\n",
        "      count += 1\n",
        "  \n",
        "  #create the predict function\n",
        "  def predict(self, X):\n",
        "    #initialise an array to hold the predicted results\n",
        "    Y_prediction = []\n",
        "    for x in X:\n",
        "      #calculate the approximation\n",
        "      linear_calc = self.weighted_sum(x)\n",
        "      #determine whether the perceptron will be activated based on the calculated approximation\n",
        "      prediction = self.activation_func(linear_calc)\n",
        "      #append the predicted results to the array and return the array\n",
        "      Y_prediction.append(prediction)\n",
        "    return np.array(Y_prediction)\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "6c6g6gujMi4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test the Perceptron on the Toy Dataset"
      ],
      "metadata": {
        "id": "lfJipOhlJ5db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#construct perceptron\n",
        "perceptron = Perceptron()\n",
        "#fit data to the perceptron\n",
        "perceptron.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "qEROK8_9J9Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run the perceptron's predict function on the test data\n",
        "pred = perceptron.predict(X_test)\n",
        "#show the predicted values and actual values\n",
        "print(\"predictions\", pred)\n",
        "print(\"actual outcomes\", y_test)\n",
        "#display the accuracy of the model on the toy dataset\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "id": "ccIPHWWmKKS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the Perecptron on the Real-World Dataset"
      ],
      "metadata": {
        "id": "wDK0Y_yq1Tqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#construct perceptron with 100 epochs instead of 10\n",
        "perceptron = Perceptron(0.1, 100)\n",
        "#fit data to the perceptron\n",
        "perceptron.fit(train_X, train_Y)"
      ],
      "metadata": {
        "id": "dnd3TPviai4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation of the Perceptron\n"
      ],
      "metadata": {
        "id": "LKQzZz8s1aSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run the perceptron's predict function on the test data\n",
        "pred = perceptron.predict(test_X)\n",
        "#check the shape of the prediction array to ensure the prediction function produced the right format\n",
        "pred.shape"
      ],
      "metadata": {
        "id": "SAoVmcDl0gxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compare the predicted results with the true results \n",
        "print(\"predictions\", pred)\n",
        "print(\"actual outcomes\", test_Y)\n",
        "print(\"confusion matrix:\")\n",
        "confusion_matrix(test_Y, pred)"
      ],
      "metadata": {
        "id": "QqDvlRaAf-8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", accuracy_score(test_Y, pred))\n",
        "print(\"Precision:\", precision_score(test_Y, pred))\n",
        "print(\"F1 score:\", f1_score(test_Y, pred))\n",
        "print(\"Area Under the Curve Score:\", roc_auc_score(test_Y, pred))"
      ],
      "metadata": {
        "id": "MK7nXXPsI903"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}